{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6701f8-424e-4dbb-a208-59f232970940",
   "metadata": {},
   "source": [
    "# Classroom 3 - Working with word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5374aa-257e-4ec3-add4-cbe26f926350",
   "metadata": {},
   "source": [
    "So far we've seen a couple of key Python libraries for doing specific tasks in NLP. For example, ```scikit-learn``` provides a whole host of fundamental machine learning algortithms; ```spaCy``` allows us to do robust linguistic analysis; ```huggingface``` is the place to go for pretrained models (more on that in coming weeks); ```pytorch``` is the best framework for building complex deep learning models.\n",
    "\n",
    "Today, we're going to meet ```gensim``` which is the best way to work with (static) word embeddings like word2vec. You can find the documentation [here](https://radimrehurek.com/gensim/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377bfa0c-a4fc-4b31-9e20-aeba254db6d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:26:12.530529Z",
     "iopub.status.busy": "2022-10-13T11:26:12.529804Z",
     "iopub.status.idle": "2022-10-13T11:26:13.556862Z",
     "shell.execute_reply": "2022-10-13T11:26:13.555130Z",
     "shell.execute_reply.started": "2022-10-13T11:26:12.530471Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wibe/Desktop/CogSci/NLP/NLP-AU-23/nlp_course_venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11e678-a148-42f4-983d-e71e2a5fa9ab",
   "metadata": {},
   "source": [
    "## Choose a language\n",
    "\n",
    "I've downloaded a number of pretrained word2vec models for different languages. Feel free to experiment with a couple (or with other models, if you want to download more: you can also download FastText embeddings: https://fasttext.cc/), but make sure to use different variable names for the models.\n",
    "\n",
    "NB: The English embeddings are 300d; all other word2vec models here are 100d. Notice also that different word2vec models are loaded in different ways. This is due to way that they were saved after training - the saved formats are not consistently the same.\n",
    "\n",
    "**Note**: depending on where your notebook is located, you may need to change the paths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed41b0a-a40c-4bc5-b98a-e87d79c05d0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:26:15.090456Z",
     "iopub.status.busy": "2022-10-13T11:26:15.089708Z",
     "iopub.status.idle": "2022-10-13T11:26:17.622369Z",
     "shell.execute_reply": "2022-10-13T11:26:17.620582Z",
     "shell.execute_reply.started": "2022-10-13T11:26:15.090393Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "# Danish embeddings https://korpus.dsl.dk/resources/details/word2vec.html\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format(\"models/danish.bin\", binary=True)\n",
    "\n",
    "# Polish embeddings https://github.com/sdadas/polish-nlp-resources#word2vec\n",
    "#model = gensim.models.KeyedVectors.load(\"models/polish/model.bin\")\n",
    "\n",
    "# English embeddings http://vectors.nlpl.eu/repository/ (English CoNLL17 corpus)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"models/english/model.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf79327-ffe5-43ba-8f09-3ee8e4ec3c95",
   "metadata": {},
   "source": [
    "I've outlined a couple of tasks for you below to experiment with. Use these just a stepping off points to explore the nature of word embeddings and how they work.\n",
    "\n",
    "Work in small groups on these tasks and make sure to discuss the issues and compare results - preferably across languages!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d604e11-9b07-4d12-b10f-3309484819fa",
   "metadata": {},
   "source": [
    "### Task 1: Finding polysemy\n",
    "\n",
    "Find a polysemous word (for example, \"leaves\" or \"scoop\") such that the top-10 most similar words (according to cosine similarity) contains related words from both meanings. An example is given for you below in English. \n",
    "\n",
    "Are there certain words for which polysemy is more of a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd074b4-23ee-4d70-afc8-db85bd53e904",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:26:39.457702Z",
     "iopub.status.busy": "2022-10-13T11:26:39.457000Z",
     "iopub.status.idle": "2022-10-13T11:26:39.620121Z",
     "shell.execute_reply": "2022-10-13T11:26:39.618193Z",
     "shell.execute_reply.started": "2022-10-13T11:26:39.457645Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('turns', 0.9376382231712341),\n",
       " ('stands', 0.9167803525924683),\n",
       " ('takes', 0.915598452091217),\n",
       " ('breaks', 0.9122215509414673),\n",
       " ('holding', 0.9075874090194702),\n",
       " ('sees', 0.906860888004303),\n",
       " ('runs', 0.9057332873344421),\n",
       " ('walks', 0.904651403427124),\n",
       " ('ground', 0.9041924476623535),\n",
       " ('holds', 0.9032379388809204)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"leaves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78f14e-45b0-4538-ae54-ffeb01836618",
   "metadata": {},
   "source": [
    "### Task 2: Synonyms and antonyms\n",
    "\n",
    "In the lecture, we saw that _cosine similarity_ can also be thought of as _cosine distance_, which is simply ```1 - cosine similarity```. So the higher the cosine distance, the further away two words are from each other and so they have less \"in common\".\n",
    "\n",
    "Find three words ```(w1,w2,w3)``` where ```w1``` and ```w2``` are synonyms and ```w1``` and ```w3``` are antonyms, but where: \n",
    "\n",
    "```Cosine Distance(w1,w3) < Cosine Distance(w1,w2)```\n",
    "\n",
    "For example, w1=\"happy\" is closer to w3=\"sad\" than to w2=\"cheerful\".\n",
    "\n",
    "Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened. Are there any inconsistencies?\n",
    "\n",
    "You should use the the ```model.distance(w1, w2)``` function here in order to compute the cosine distance between two words. I've given a starting example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b476719-0cc1-4da2-bdd9-98bd22bd48e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:28:51.002030Z",
     "iopub.status.busy": "2022-10-13T11:28:51.001364Z",
     "iopub.status.idle": "2022-10-13T11:28:51.012787Z",
     "shell.execute_reply": "2022-10-13T11:28:51.011701Z",
     "shell.execute_reply.started": "2022-10-13T11:28:51.001973Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2629561424255371"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.distance(\"happy\", \"sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72d9efe2-809f-4944-9e6d-a9c735ccc4ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:28:36.667432Z",
     "iopub.status.busy": "2022-10-13T11:28:36.666728Z",
     "iopub.status.idle": "2022-10-13T11:28:36.677467Z",
     "shell.execute_reply": "2022-10-13T11:28:36.676303Z",
     "shell.execute_reply.started": "2022-10-13T11:28:36.667375Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5665353238582611"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.distance(\"happy\",\"cheerful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ed87ba7-851c-42bb-91a1-0033c6ebaa79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.distance(\"happy\", \"sad\") < model.distance(\"happy\",\"cheerful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86210994-55e0-4ef1-99c8-9345e297cbc9",
   "metadata": {},
   "source": [
    "### Task 3: Word analogies\n",
    "\n",
    "We saw in the lecture on Wednesday that we can use basic arithmetic on word embeddings, in order to conduct word analogy task.\n",
    "\n",
    "For example:\n",
    "\n",
    "```man::king as woman::queen```\n",
    "\n",
    "So we can say that if we take the vector for ```king``` and subtract the vector for ```man```, we're removing the gender component from the ```king```. If we then add ```woman``` to the resulting vector, we should be left with a vector similar to ```queen```.\n",
    "\n",
    "NB: It might not be _exactly_ the vector for ```queen```, but it should at least be _close_ to it.\n",
    "\n",
    "```gensim``` has some quirky syntax that allows us to perform this kind of arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f38cc-a6a6-4d54-a0ef-97b90bef4140",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:37:01.747364Z",
     "iopub.status.busy": "2022-10-13T11:37:01.746670Z",
     "iopub.status.idle": "2022-10-13T11:37:01.859900Z",
     "shell.execute_reply": "2022-10-13T11:37:01.858453Z",
     "shell.execute_reply.started": "2022-10-13T11:37:01.747307Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['king', 'woman'], \n",
    "                   negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8a954-4feb-4b5e-894b-f530d9bf96de",
   "metadata": {},
   "source": [
    "Try to find at least three analogies which correctly hold - where \"correctly\" here means that the closest vector corresponds to the word that you as a native speaker think it should."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6632d94f-d2c1-4bfb-aaf8-92c2a26c2fab",
   "metadata": {},
   "source": [
    "### Task 3b: Wrong analogies\n",
    "\n",
    "Can you find any analogies which _should_ hold but don't? Why don't they work? Are there any similarities or trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8721d5-1b49-4f67-89dc-cdea12114722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "378e672c-9140-49b6-91fa-f8d5364a91f6",
   "metadata": {},
   "source": [
    "### Task 4: Exploring bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217f907d-5ecb-4f8a-93b1-e4c19f67e3d0",
   "metadata": {},
   "source": [
    "As we spoke briefly about in the lecture, word embeddings tend to display bias of the kind found in the training data.\n",
    "\n",
    "Using some of the techniques you've worked on above, can you find some clear instances of bias in the word embedding models that you're exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2caac-f064-4ee8-8cc4-c81690da786a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:38:45.594622Z",
     "iopub.status.busy": "2022-10-13T11:38:45.593924Z",
     "iopub.status.idle": "2022-10-13T11:38:45.707115Z",
     "shell.execute_reply": "2022-10-13T11:38:45.705573Z",
     "shell.execute_reply.started": "2022-10-13T11:38:45.594565Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['doctor', 'woman'], \n",
    "                   negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c9179c-307a-4c7b-b3a0-6e0316df4f83",
   "metadata": {},
   "source": [
    "### Task 5: Dimensionality reduction and visualizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1419a1e5-a8be-44df-9334-f03dc427122c",
   "metadata": {},
   "source": [
    "In the following cell, I've written a short bit of code which takes a given subset of words and plots them on a simple scatter plot. Remember that the word embeddings are 300d (or 100d here, depending on which language you're using), so we need to perform some kind of dimensionality reduction on the embeddings to get them down to 2D.\n",
    "\n",
    "Here, I'm using a simply PCA algorithm implemented via ```scikit-learn```. An alternative approach might also be to use Singular Value Decomposition or SVD, which works in a similar but ever-so-slightly different way to PCA. You can read more [here](https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/) and [here](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491) - the maths is bit mind-bending, just FYI.\n",
    "\n",
    "Experiment with plotting certain subsets of words by changing the ```words``` list. How useful do you find these plots? Do they show anything meaningful?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2555a971-2538-416b-b3c6-7c1732893d2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T12:10:20.405678Z",
     "iopub.status.busy": "2022-10-13T12:10:20.404976Z",
     "iopub.status.idle": "2022-10-13T12:10:20.570076Z",
     "shell.execute_reply": "2022-10-13T12:10:20.569514Z",
     "shell.execute_reply.started": "2022-10-13T12:10:20.405623Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the list of words we want to plot\n",
    "words = [\"man\", \"woman\", \"doctor\", \"nurse\", \"king\", \"queen\", \"boy\", \"girl\"]\n",
    "\n",
    "# an empty list for vectors\n",
    "X = []\n",
    "# get vectors for subset of words\n",
    "for word in words:\n",
    "    X.append(model[word])\n",
    "\n",
    "# Use PCA for dimensionality reduction to 2D\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# or try SVD - how are they different?\n",
    "#svd = TruncatedSVD(n_components=2)\n",
    "# fit_transform the initialized PCA model\n",
    "#result = svd.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "\n",
    "# for each word in the list of words\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831a495",
   "metadata": {},
   "source": [
    "### Bonus tasks\n",
    "\n",
    "If you run out of things to explore with these embeddings, try some of the following tasks:\n",
    "\n",
    "[Easier]\n",
    "- make new plots like those above but cleaner and more informative\n",
    "- write a script which takes a list of words and produces the output above\n",
    "  \n",
    "[Very advanced]\n",
    "- work through [this](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html) documentation which demonstrates how to train word embedding using ```pytorch```. Compare this to the training documentation [here](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html) and think about how you would train a larger model on your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3eebba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdeeb5c9",
   "metadata": {},
   "source": [
    "# Training a word2vec from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45882d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1714d0c30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84af6827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4e65bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.\n",
    "# Each tuple is ([ word_i-CONTEXT_SIZE, ..., word_i-1 ], target word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b8dbe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['forty', 'When'], 'winters'), (['winters', 'forty'], 'shall'), (['shall', 'winters'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "ngrams = [\n",
    "    (\n",
    "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        test_sentence[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
    "]\n",
    "# Print the first 3, just so you can see what they look like.\n",
    "print(ngrams[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "388a7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "367d4e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd8d3dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[521.2370474338531, 518.5287251472473, 515.8427066802979, 513.177051782608, 510.53172421455383, 507.90635800361633, 505.30082082748413, 502.7138707637787, 500.1435751914978, 497.5883078575134]\n",
      "tensor([-0.2158,  0.3304, -0.9346,  1.6579,  1.0070,  1.3937, -1.7941,  1.3686,\n",
      "         0.0217,  0.9818], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!\n",
    "\n",
    "# To get the embedding of a particular word, e.g. \"beauty\"\n",
    "print(model.embeddings.weight[word_to_ix[\"beauty\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a1d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315887aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
